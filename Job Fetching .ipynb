{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fetching multiple pages and in that we will fetch multiple cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title = []\n",
    "Company =[]\n",
    "Location =[]\n",
    "Salary = []\n",
    "Discription = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collector():   \n",
    "    browser = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    for i in range(0,10,10):     #as we see the link it start from 0 and at every next page 0 turns into 10 so we use 10 increment so we are fetching 50 pages\n",
    "            browser.get(\"https://www.indeed.co.in/jobs?q=machine+learning&l=India&start={}\".format(i))\n",
    "            Cards = browser.find_elements_by_class_name(\"result\") #to fetch multiple tags with same class we use elements\n",
    "            for card in Cards:\n",
    "                #whole card detail\n",
    "                details= BeautifulSoup(card.get_attribute(\"innerHTML\"),\"html.parser\")\n",
    "\n",
    "                # we are using this try beacuse a pop up comes on the window and we to close that to avoid error\n",
    "                try:\n",
    "                    if browser.find_element_by_id(\"popover-foreground\"):\n",
    "                        cross = browser.find_element_by_xpath(\"//div[@id='popover-foreground']/div[@id='popover-x']/a\").click()\n",
    "\n",
    "\n",
    "                except:\n",
    "                    #we are using try beause if title is not there then we dont get error \n",
    "                    try:\n",
    "                        title = details.find(\"a\",class_=\"jobtitle\")\n",
    "                        Title.append(title.text.replace(\"\\n\",\"\"))\n",
    "                    except:\n",
    "                        Title.append(None)\n",
    "\n",
    "                #company name\n",
    "                    try:\n",
    "                        company = details.find(\"span\",class_=\"company\")\n",
    "                        Company.append(company.text.replace(\"\\n\",\"\"))\n",
    "                    except:\n",
    "                        Company.append(None)\n",
    "\n",
    "                #location\n",
    "                    try:\n",
    "                        location = details.find(\"div\",class_=\"location\")\n",
    "                        Location.append(location.text.replace(\"\\n\",\"\"))\n",
    "                    except:\n",
    "                        Location.append(None)\n",
    "\n",
    "                #salary\n",
    "                    try:\n",
    "                        salary = details.find(\"span\",class_=\"salaryText\")\n",
    "                        Salary.append(salary.text.replace(\"\\n\",\"\"))\n",
    "                    except:\n",
    "                        Salary.append(None)\n",
    "\n",
    "                #now we have to fetch the discription so we have to click on card and then fetch the tag\n",
    "                    card.click()\n",
    "                    sleep(1)\n",
    "\n",
    "                    try: \n",
    "                        des = browser.find_element_by_id(\"vjs-desc\")\n",
    "                        all_li_tag = BeautifulSoup(des.get_attribute(\"innerHTML\"),\"html.parser\")\n",
    "                        all_li_dis =[]\n",
    "                        all_li_tag = all_li_tag.find_all(\"li\")\n",
    "                        for li in all_li_tag:\n",
    "                            all_li_dis.append(li.text.replace(\"\\n\",\"\"))\n",
    "                        all_li_dis = \" \".join(all_li_dis)\n",
    "                        Discription.append(all_li_dis)\n",
    "\n",
    "                    except:\n",
    "                        Discription.append(None)\n",
    "\n",
    "#......when we need we all this ...................------->>>>>>  collector() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the data we fetch throudh code                 #we fetch multiple data with this \n",
    "df = pd.DataFrame({\n",
    "    \"Title\":Title,\n",
    "    \"Company\":Company,\n",
    "    \"Location\":Location,\n",
    "    \"Salary\":Salary,\n",
    "    \"Details\":Discription\n",
    "})\n",
    " f.to_csv(\"./CSV_From_Scratch/MachineLearningJobs.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
